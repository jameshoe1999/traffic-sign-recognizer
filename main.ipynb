{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Traffic Sign Recognition using Histogram of Oriented Gradient and Convolutional Neural Network\r\n",
    "This notebook will showcase the training process of the traffic sign recognizer using neural network. Before you click Run All, it is important to check that the following packages were installed on your host machine.\r\n",
    "  1. opencv-python\r\n",
    "  2. tensorflow\r\n",
    "  3. numpy\r\n",
    "  4. scikit-image\r\n",
    "  5. scipy\r\n",
    "\r\n",
    "## Move your dataset to the src folder\r\n",
    "Note that you should rename the files of a type as \"XXXX_\" where XXXX is the label of the image,\r\n",
    "the label should not be more than 4 characters!\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Acqusition \r\n",
    "Loading the dataset along with the respective labels\r\n",
    "If the dataset has one __80 speed limit__ sign and another __stop__ sign,\r\n",
    "the labels will be ['80', 'Stop']\r\n",
    "__Note__: The size of data and label should be the same, report to us if you find the size was different."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import common as cm\r\n",
    "\r\n",
    "cm.files_rename('src')\r\n",
    "data, labels = cm.load_data('src')\r\n",
    "print('Size of Data:', len(data))\r\n",
    "print('Size of Label:', len(labels))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing Stage\r\n",
    "Every image in the dataset will undergo the preprocessing stage which included the following operations:\r\n",
    "  1. Histogram Normalization - to enhance the constrast and detail of an image\r\n",
    "  2. Hough Transform - to detect the largest circle in the image and return the content within the circle, it was set to accept not so round shape as well.\r\n",
    "  3. HOG feature descriptor - to extract the feature from the images, features as in the change in gradient.\r\n",
    "  4. Reshaping - reshape the image to (*image.shape, 1) for the classifier\r\n",
    "You may notice the data size was reduced to a lower number.\r\n",
    "This is because the preprocessor will filter out those photos in awful quality, or when the Hough transform failed to detect round shape on the image."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "from parallel import preprocess_image_parallel as preprocess\r\n",
    "\r\n",
    "dataset, label = preprocess(data, labels) # dataset\r\n",
    "data_size = len(dataset)\r\n",
    "data_type = type(dataset)\r\n",
    "print(f'''\r\n",
    "Data size: {data_size}\r\n",
    "Data type: {data_type}\r\n",
    "''')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The following code shows the sizes of each type of traffic signs.\r\n",
    "Depending on the traffic signs dataset you downloaded, the sizes of each type of traffic signs can be different.\r\n",
    "The counting was presented as the following format:\r\n",
    "(name_of_sign, size_of_sign)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import collections\r\n",
    "\r\n",
    "counter = collections.Counter(label)\r\n",
    "counter.most_common()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preprocessing Stage 2\r\n",
    "It is required to convert the image dataset to numpy array for the classifier to work with.\r\n",
    "For the label, we first converted them into numeric form ('80' -> 0, 'SP' -> 1, etc.)\r\n",
    "Then, the label was categorized into the CNN preferred format which is:\r\n",
    "say there is 5 types of sign, and the first sign is __80 speed limit sign__ which is the index 0 after the conversion will become\r\n",
    "  [1, 0, 0, 0, 0]\r\n",
    "where the remaining 4 zeros are reserved for the other 4 signs."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.utils import to_categorical\r\n",
    "from json import dumps\r\n",
    "\r\n",
    "X = np.array(dataset)\r\n",
    "\r\n",
    "print('X shape:', X.shape)\r\n",
    "\r\n",
    "label_set = set(label)\r\n",
    "classes = { val: key for (key, val) in enumerate(label_set)}\r\n",
    "# cache the label types for testing purpose\r\n",
    "output_text = dumps(classes)\r\n",
    "output = open('classes.txt', 'w')\r\n",
    "output.write(output_text)\r\n",
    "output.close()\r\n",
    "print(classes)\r\n",
    "\r\n",
    "Y = np.fromiter([classes[y] for y in label], dtype=np.int)\r\n",
    "Y = to_categorical(Y)\r\n",
    "dense = len(Y[0])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data splitting\r\n",
    "We need to separate part of the data for testing purpose."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\r\n",
    "X_train.shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Contruct the layers of our CNN model."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Dense, Flatten\r\n",
    "\r\n",
    "# initiate model\r\n",
    "model = Sequential()\r\n",
    "\r\n",
    "# add model layers\r\n",
    "model.add(Conv2D(32, (4, 4), activation='relu', input_shape=(512, 256, 1)))\r\n",
    "model.add(MaxPool2D((2, 2)))\r\n",
    "model.add(Conv2D(64, (4, 4), activation='relu'))\r\n",
    "model.add(MaxPool2D((2, 2)))\r\n",
    "model.add(Conv2D(128, (4, 4), activation='relu'))\r\n",
    "model.add(Flatten())\r\n",
    "model.add(Dense(dense, activation='softmax'))\r\n",
    "model.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# compile model using accuracy to measure the performance\r\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train the model\r\n",
    "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Never forget to save the model!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save('saved_models/my_model')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "22d9742e846c5b2e6352a52abbfdb0eaab0b509899a61abbfb2417673253b584"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}